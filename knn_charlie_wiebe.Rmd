---
title: $K$NN
author: Charlie Wiebe
date: "02/10/2025"

format: 
  html:  # You will quite likely want to change all but the last one, to taste
    theme: superhero  
    mainfont: monospace
    highlight-style: github
    title-block-banner: true
    embed-resources: true

---

**Abstract:**

This is a technical blog post of **both** an HTML file *and* [.qmd file](https://raw.githubusercontent.com/cd-public/D505/refs/heads/master/hws/src/knn.qmd) hosted on GitHub pages.

# 0. Quarto Type-setting

- This document is rendered with Quarto, and configured to embed an images using the `embed-resources` option in the header.
- If you wish to use a similar header, here's is the format specification for this document:

```email
format: 
  html:
    embed-resources: true
```

# 1. Setup

```{r}
library(tidyverse)
library(caret)
wine <- readRDS(gzcon(url("https://github.com/cd-public/D505/raw/master/dat/pinot.rds")))
```

## 2. $K$NN Concepts

> <span style="color:red;font-weight:bold">TODO</span>: *Explain how the choice of K affects the quality of your prediction when using a $K$ Nearest Neighbors algorithm.*

## 3. Feature Engineering

1. Create a version of the year column that is a *factor* (instead of numeric).
2. Create dummy variables that indicate the presence of "cherry", "chocolate" and "earth" in the description.
  - Take care to handle upper and lower case characters.
3. Create 3 new features that represent the interaction between *time* and the cherry, chocolate and earth inidicators.
4. Remove the description column from the data.

```{r}
library(stringr)

wine <- wine %>%
  mutate(year_factor = as.factor(year),
         cherry_indicator = as.integer(str_detect(description, regex("cherry", ignore_case = TRUE))),
         chocolate_indicator = as.integer(str_detect(description, regex("chocolate", ignore_case = TRUE))),
         earth_indicator = as.integer(str_detect(description, regex("earth", ignore_case = TRUE))),
         cherry_time_interaction = as.integer(year) * cherry_indicator,
         chocolate_time_interaction = as.integer(year) * chocolate_indicator,
         earth_time_interaction = as.integer(year) * earth_indicator) %>%

  select(-description)


head(wine)
```
## 4. Preprocessing

1. Preprocess the dataframe from the previous code block using BoxCox, centering and scaling of the numeric features
2. Create dummy variables for the `year` factor column

```{r}
library(dplyr)
library(caret)

wine_preprocessed <- wine %>%
  preProcess(method = c("BoxCox", "center", "scale")) %>%
  predict(wine) %>%
  bind_cols(
    .,
    predict(dummyVars(~ year_factor, data = .), newdata = .)
  ) %>%
  select(-year_factor)

head(wine_preprocessed)
```


## 5. Running $K$NN

1. Split the dataframe into an 80/20 training and test set
2. Use Caret to run a $K$NN model that uses our engineered features to predict province
  - use 5-fold cross validated subsampling 
  - allow Caret to try 15 different values for $K$
3. Display the confusion matrix on the test data


```{r}
set.seed(123)  # For reproducibility
train_index <- createDataPartition(wine_preprocessed$province, p = 0.8, list = FALSE)
train_data <- wine_preprocessed[train_index, ]
test_data <- wine_preprocessed[-train_index, ]

train_control <- trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE
)

knn_model <- train(
  province ~ .,
  data = train_data,
  method = "knn",
  trControl = train_control,
  tuneLength = 15
)

print(knn_model)

test_data$province <- as.factor(test_data$province)

test_predictions <- predict(knn_model, newdata = test_data)

test_predictions <- factor(test_predictions, levels = levels(test_data$province))

confusion_matrix <- confusionMatrix(test_predictions, test_data$province)
print(confusion_matrix)
```

## 6. Kappa

How do we determine whether a Kappa value represents a good, bad or some other outcome?

> <span style="color:red;font-weight:bold">TODO</span>: A good rule of thumb is less than .2 is bad, .21 to .4 is okay, .41-.6 is pretty good, .61-.8 is great, and anything higher is almost perfect. The highest kappa value I got here was 0.391 (with k=19), which is on the upper end of okay.

## 7. Improvement

How can we interpret the confusion matrix, and how can we improve in our predictions?

> <span style="color:red;font-weight:bold">TODO</span>: The model performs quite poorly on minority classes. We can fix this by over/undersampling (either randomly duplicating values in minority classes or randomly removing values in the majority classes).